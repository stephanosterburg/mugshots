{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.0.0-beta1 in /anaconda3/lib/python3.7/site-packages (2.0.0b1)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (0.1.7)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.21.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.15.4)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (0.33.4)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
      "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (3.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (0.8.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.9.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.14.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /anaconda3/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (41.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__ is not \"2.0.0-beta1\":\n",
    "    !pip install tensorflow==2.0.0-beta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import h5py\n",
    "import skimage\n",
    "import numpy as np\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    hdf5_path = 'data/dataset.hdf5'\n",
    "    hdf5_file = h5py.File(hdf5_path, 'r')\n",
    "    \n",
    "    X_train = hdf5_file['train_inpt'][:]\n",
    "    y_train = hdf5_file['train_real'][:]\n",
    "    X_test = hdf5_file['test_inpt'][:]\n",
    "    y_test = hdf5_file['test_real'][:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_source_images, train_target_images, test_source_images, test_target_images = load_dataset()\n",
    "\n",
    "train_source_images = train_source_images.reshape(train_source_images.shape[0], 256, 256, 3).astype('float32')\n",
    "train_source_images = train_source_images / 255.\n",
    "train_target_images = train_target_images.reshape(train_target_images.shape[0], 256, 256, 3).astype('float32')\n",
    "train_target_images = train_target_images / 255.\n",
    "test_source_images = test_source_images.reshape(test_source_images.shape[0], 256, 256, 3).astype('float32')\n",
    "test_source_images = test_source_images / 255.\n",
    "test_target_images = test_target_images.reshape(test_target_images.shape[0], 256, 256, 3).astype('float32')\n",
    "test_target_images = test_target_images / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3892, 256, 256, 3)\n",
      "(244, 256, 256, 3)\n",
      "(3892, 256, 256, 3)\n",
      "(244, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Check dataset sizes\n",
    "print(train_source_images.shape)\n",
    "print(train_target_images.shape)\n",
    "print(test_source_images.shape)\n",
    "print(test_target_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 400  # train_source_images.shape[0] # number of training images\n",
    "BATCH_SIZE = 4 # training batch size (memory dependent)\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch the data. I've eliminated shuffling here.\n",
    "train_source_dataset = tf.data.Dataset.from_tensor_slices(train_source_images).batch(BATCH_SIZE)\n",
    "train_target_dataset = tf.data.Dataset.from_tensor_slices(train_target_images).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "The Image-to-Image paper notes that it uses the U-Net as the generator. \n",
    "\n",
    "![U-Net](u-net-architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generator(input_shape):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inputs = tf.keras.Input(input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    # Encoder network\n",
    "    x = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv0')(x)\n",
    "    conv0 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    # Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv1')(conv0)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv1 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv2')(conv1)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv2 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv3')(conv2)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv3 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv4')(conv3)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv4 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv5')(conv4)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv5 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv6')(conv5)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv6 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # tf.keras.layers.Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='enc_conv7')(conv6)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    conv7 = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Fully-connected layers to allow parts to move around.\n",
    "    # If you are running out of memory, you can comment some of them out.\n",
    "\n",
    "    # Flatten -> Dense -> LeakyReLU -> Dense -> LeakyReLU -> Dense -> LeakyReLU -> Dense -> LeakyReLU -> Dense -> LeakyReLU -> Reshape\n",
    "    x = tf.keras.layers.Flatten()(conv7)\n",
    "    x = tf.keras.layers.Dense(512, input_shape=(1, 1, 512), name='dense1')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, input_shape=(1, 1, 512), name='dense2')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, input_shape=(1, 1, 512), name='dense3')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, input_shape=(1, 1, 512), name='dense4')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(512, input_shape=(1, 1, 512), name='dense5')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    x = tf.keras.layers.Reshape((1, 1, 512))(x)\n",
    "\n",
    "    # Decoder network\n",
    "    # tf.keras.layers.Conv2DTrans -> BatchNorm -> Dropout -> ReLU\n",
    "    x = tf.keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv7')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> Dropout -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat6')([conv6, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv6')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> Dropout -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat5')([conv5, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv5')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.Dropout(rate=0.5)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat4')([conv4, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(1024, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv4')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat3')([conv3, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(512, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv3')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=-1)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat2')([conv2, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(256, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> BatchNorm -> ReLU\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat1')([conv1, x])\n",
    "    x = tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', \n",
    "                               kernel_initializer=initializer, \n",
    "                               use_bias=False, name='dec_conv1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.ReLU()(x)\n",
    "\n",
    "    # Concat -> tf.keras.layers.Conv2DTrans -> TanH\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat0')([conv0, x])\n",
    "    outputs = tf.keras.layers.Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same',\n",
    "                                              kernel_initializer=initializer,\n",
    "                                              use_bias=False, activation='tanh', \n",
    "                                              name='dec_conv0')(x)\n",
    "\n",
    "    # Return model. \n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name='Generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv0 (Conv2D)              (None, 128, 128, 64) 3072        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 128, 128, 64) 0           enc_conv0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv1 (Conv2D)              (None, 64, 64, 128)  131072      leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 64, 64, 128)  512         enc_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 64, 64, 128)  0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv2 (Conv2D)              (None, 32, 32, 256)  524288      leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 256)  1024        enc_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 256)  0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv3 (Conv2D)              (None, 16, 16, 512)  2097152     leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 16, 16, 512)  2048        enc_conv3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 16, 16, 512)  0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv4 (Conv2D)              (None, 8, 8, 512)    4194304     leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 8, 8, 512)    2048        enc_conv4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 8, 8, 512)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv5 (Conv2D)              (None, 4, 4, 512)    4194304     leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 4, 4, 512)    2048        enc_conv5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 4, 4, 512)    0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv6 (Conv2D)              (None, 2, 2, 512)    4194304     leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 2, 2, 512)    2048        enc_conv6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 2, 2, 512)    0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "enc_conv7 (Conv2D)              (None, 1, 1, 512)    4194304     leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1, 1, 512)    2048        enc_conv7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 1, 1, 512)    0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 512)          262656      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 512)          0           dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 512)          262656      leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 512)          0           dense2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense3 (Dense)                  (None, 512)          262656      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 512)          0           dense3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense4 (Dense)                  (None, 512)          262656      leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 512)          0           dense4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense5 (Dense)                  (None, 512)          262656      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 512)          0           dense5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1, 1, 512)    0           leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv7 (Conv2DTranspose)     (None, 2, 2, 1024)   8388608     reshape[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 2, 2, 1024)   4096        dec_conv7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 2, 2, 1024)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, 2, 2, 1024)   0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concat6 (Concatenate)           (None, 2, 2, 1536)   0           leaky_re_lu_6[0][0]              \n",
      "                                                                 re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv6 (Conv2DTranspose)     (None, 4, 4, 1024)   25165824    concat6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 4, 4, 1024)   4096        dec_conv6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 4, 4, 1024)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, 4, 4, 1024)   0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat5 (Concatenate)           (None, 4, 4, 1536)   0           leaky_re_lu_5[0][0]              \n",
      "                                                                 re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv5 (Conv2DTranspose)     (None, 8, 8, 1024)   25165824    concat5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 8, 8, 1024)   4096        dec_conv5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 8, 1024)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, 8, 8, 1024)   0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concat4 (Concatenate)           (None, 8, 8, 1536)   0           leaky_re_lu_4[0][0]              \n",
      "                                                                 re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv4 (Conv2DTranspose)     (None, 16, 16, 1024) 25165824    concat4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 1024) 4096        dec_conv4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, 16, 16, 1024) 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat3 (Concatenate)           (None, 16, 16, 1536) 0           leaky_re_lu_3[0][0]              \n",
      "                                                                 re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv3 (Conv2DTranspose)     (None, 32, 32, 512)  12582912    concat3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 512)  2048        dec_conv3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 32, 32, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat2 (Concatenate)           (None, 32, 32, 768)  0           leaky_re_lu_2[0][0]              \n",
      "                                                                 re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv2 (Conv2DTranspose)     (None, 64, 64, 256)  3145728     concat2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 64, 64, 256)  1024        dec_conv2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 64, 64, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat1 (Concatenate)           (None, 64, 64, 384)  0           leaky_re_lu_1[0][0]              \n",
      "                                                                 re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv1 (Conv2DTranspose)     (None, 128, 128, 128 786432      concat1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 128, 128, 128 512         dec_conv1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 128, 128, 128 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concat0 (Concatenate)           (None, 128, 128, 192 0           leaky_re_lu[0][0]                \n",
      "                                                                 re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dec_conv0 (Conv2DTranspose)     (None, 256, 256, 3)  9216        concat0[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 121,288,192\n",
      "Trainable params: 121,272,320\n",
      "Non-trainable params: 15,872\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = Generator((256, 256, 3))\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "The discriminator takes the input shape of the image file and in our case it's (256, 256, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator(source_shape, target_shape):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    input_image = tf.keras.Input(source_shape, name='input_image')\n",
    "    target_image = tf.keras.Input(target_shape, name='target_image')\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1, name='concat')([input_image, target_image])\n",
    "\n",
    "    # Conv -> LeakyReLU\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=1, data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='valid', use_bias=False,\n",
    "                               kernel_initializer=initializer, name='disc_conv0')(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=1, data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='valid', use_bias=False,\n",
    "                               kernel_initializer=initializer, name='disc_conv1')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=1, data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Conv2D(256, (4, 4), strides=(2, 2), padding='valid', use_bias=False,\n",
    "                               kernel_initializer=initializer, name='disc_conv2')(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Conv -> BatchNorm -> LeakyReLU\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=1, data_format='channels_last')(x)\n",
    "    x = tf.keras.layers.Conv2D(512, (4, 4), strides=(1, 1), name='disc_conv3', \n",
    "                               kernel_initializer=initializer, use_bias=False)(x)\n",
    "    x = tf.keras.layers.BatchNormalization(axis=3)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Conv -> Sigmoid\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=1, data_format='channels_last')(x)\n",
    "    outputs = tf.keras.layers.Conv2D(1, (4, 4), strides=(1, 1), name='validity', use_bias=False,\n",
    "                                     kernel_initializer=initializer, activation='sigmoid')(x)\n",
    "\n",
    "    # Return model\n",
    "    return tf.keras.Model(inputs=[input_image, target_image], outputs=outputs, name='Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Discriminator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "target_image (InputLayer)       [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concat (Concatenate)            (None, 256, 256, 6)  0           input_image[0][0]                \n",
      "                                                                 target_image[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 258, 258, 6)  0           concat[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "disc_conv0 (Conv2D)             (None, 128, 128, 64) 6144        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 128, 128, 64) 0           disc_conv0[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 130, 130, 64) 0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "disc_conv1 (Conv2D)             (None, 64, 64, 128)  131072      zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 64, 64, 128)  512         disc_conv1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 64, 64, 128)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 66, 66, 128)  0           leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "disc_conv2 (Conv2D)             (None, 32, 32, 256)  524288      zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 256)  1024        disc_conv2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 32, 32, 256)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 34, 34, 256)  0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "disc_conv3 (Conv2D)             (None, 31, 31, 512)  2097152     zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 31, 31, 512)  2048        disc_conv3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 31, 31, 512)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 33, 33, 512)  0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "validity (Conv2D)               (None, 30, 30, 1)    8192        zero_padding2d_4[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 2,770,432\n",
      "Trainable params: 2,768,640\n",
      "Non-trainable params: 1,792\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator((256, 256, 3),(256,256,3))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Loss functions for Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l1_weight and gan_weight are taken from the Image-to-Iamge paper.\n",
    "# The numbers scale the two components of the loss function in the GAN.\n",
    "l1_weight = 100.0\n",
    "gan_weight = 1.0\n",
    "\n",
    "# Epsilon\n",
    "epsilon = 1e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output): \n",
    "    total_loss = tf.reduce_mean(-(tf.math.log(real_output + epsilon) + tf.math.log(1 - fake_output + epsilon)))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, target_images, generated_images):\n",
    "    gen_loss_GAN = tf.reduce_mean(-tf.math.log(fake_output + epsilon))\n",
    "    gen_loss_L1 = tf.reduce_mean(tf.math.abs(target_images - generated_images))\n",
    "    total_loss = gen_loss_GAN * gan_weight + gen_loss_L1 * l1_weight\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.99, epsilon=epsilon)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5, beta_2=0.99, epsilon=epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = 'data/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The use of `tf.function` causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(source_images, target_images, epoch_num):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(source_images, training=True)\n",
    "\n",
    "        real_output = discriminator((source_images, target_images), training=True)\n",
    "        fake_output = discriminator((source_images, generated_images), training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, target_images, generated_images)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        report_gen_loss = tf.math.reduce_sum(gen_loss)\n",
    "        report_disc_loss = tf.math.reduce_sum(disc_loss)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "\n",
    "    return report_gen_loss, report_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(source_dataset, target_dataset, epochs):\n",
    "    epoch_start = 0\n",
    "\n",
    "    # Set up history reporting\n",
    "    history = {}\n",
    "    gen_loss_list = []\n",
    "    disc_loss_list = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        #source_iterator = source_dataset.make_one_shot_iterator()\n",
    "        source_index = 0\n",
    "        total_gen_loss = 0\n",
    "        total_disc_loss = 0\n",
    "\n",
    "        for image_target_batch in target_dataset:\n",
    "            image_source_batch = train_source_images[np.array(list(range(source_index, source_index + BATCH_SIZE)))]\n",
    "            report_gen_loss, report_disc_loss = train_step(image_source_batch, image_target_batch, epoch)\n",
    "            source_index = source_index + BATCH_SIZE\n",
    "            total_gen_loss = total_gen_loss + report_gen_loss\n",
    "            total_disc_loss = total_disc_loss + report_disc_loss\n",
    "\n",
    "        # Record and print the losses\n",
    "        total_gen_loss_adj = (total_gen_loss / BUFFER_SIZE) / l1_weight\n",
    "        total_disc_loss_adj = total_disc_loss / BUFFER_SIZE\n",
    "        gen_loss_list.append(total_gen_loss_adj)\n",
    "        disc_loss_list.append(total_disc_loss_adj)\n",
    "        \n",
    "        print(\"Generator Loss {}\".format(total_gen_loss_adj))\n",
    "        print(\"Discriminator Loss {}\".format(total_disc_loss_adj))\n",
    "        \n",
    "        history['gen_loss'] = gen_loss_list\n",
    "        history['disc_loss'] = disc_loss_list\n",
    "\n",
    "        # Save images every 5 epochs.\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            generated_image = generator(test_source_images[np.array([0])], training=False)\n",
    "            prediction = np.reshape(generated_image, (256, 256, 3))\n",
    "            final_image = np.clip((prediction * 255), 0, 255).astype(np.uint8)\n",
    "            generated_image2 = Image.fromarray(final_image)\n",
    "            generated_image2.save('data/sample_output1_{:04d}.jpg'.format(epoch + 1))\n",
    "\n",
    "            generated_image = generator(test_source_images[np.array([1])], training=False)\n",
    "            prediction = np.reshape(generated_image, (256, 256, 3))\n",
    "            final_image = np.clip((prediction * 255), 0, 255).astype(np.uint8)\n",
    "            generated_image2 = Image.fromarray(final_image)\n",
    "            generated_image2.save('data/sample_output2_{:04d}.jpg'.format(epoch + 1))\n",
    "\n",
    "            generated_image = generator(test_source_images[np.array([2])], training=False)\n",
    "            prediction = np.reshape(generated_image, (256, 256, 3))\n",
    "            final_image = np.clip((prediction * 255), 0, 255).astype(np.uint8)\n",
    "            generated_image2 = Image.fromarray(final_image)\n",
    "            generated_image2.save('data/sample_output3_{:04d}.jpg'.format(epoch + 1))\n",
    "\n",
    "            generated_image = generator(test_source_images[np.array([3])], training=False)\n",
    "            prediction = np.reshape(generated_image, (256, 256, 3))\n",
    "            final_image = np.clip((prediction * 255), 0, 255).astype(np.uint8)\n",
    "            generated_image2 = Image.fromarray(final_image)\n",
    "            generated_image2.save('data/sample_output4_{:04d}.jpg'.format(epoch + 1))\n",
    "\n",
    "        # Save the model every 20 epochs\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        # Print time every 1 epochs\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print ('Time for epoch {} is {} seconds\\n'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator Loss 0.019965488463640213\n",
      "Discriminator Loss 0.17527827620506287\n",
      "Time for epoch 1 is 322.569943189621 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d076c38ec43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_source_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-ec03bd6f163d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(source_dataset, target_dataset, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_target_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mimage_source_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_source_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mreport_gen_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_disc_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_source_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_target_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0msource_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtotal_gen_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_gen_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreport_gen_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    402\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1335\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \"\"\"\n\u001b[1;32m    588\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 589\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    590\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    591\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    443\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    444\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 445\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    446\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(train_source_dataset, train_target_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore the latest checkpoint and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "    \n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    generated_image = generator(test_source_images[np.array([i])], training=False)\n",
    "    prediction = np.reshape(generated_image, (256, 256, 3))\n",
    "    final_image = np.clip((prediction * 255), 0, 255).astype(np.uint8)\n",
    "    generated_image = Image.fromarray(final_image)\n",
    "    generated_image.save('dataset/final.{:04d}.jpg'.format(i + 1))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(generated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input, tar):\n",
    "    # the training=True is intentional here since\n",
    "    # we want the batch statistics while running the model\n",
    "    # on the test dataset. If we use training=False, we will get\n",
    "    # the accumulated statistics learned from the training dataset\n",
    "    # (which we don't want)\n",
    "    prediction = model(test_input, training=True)\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    display_list = [test_input[0], tar[0], prediction[0]]\n",
    "    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, tar in test_dataset.take(5):\n",
    "    generate_images(generator, inp, tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
